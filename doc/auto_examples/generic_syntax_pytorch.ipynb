{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nExample of KeOps reduction using the generic syntax.\n====================================================\n\nThis example uses the pyTorch framework.\n\n# this example computes the following tensor operation :\n# inputs :\n#   x   : a 3000x3 tensor, with entries denoted x_i^u\n#   y   : a 5000x3 tensor, with entries denoted y_j^u\n#   a   : a 5000x1 tensor, with entries denoted a_j\n#   p   : a scalar (entered as a 1x1 tensor)\n# output :\n#   c   : a 3000x3 tensor, with entries denoted c_i^u, such that\n#   c_i^u = sum_j (p-a_j)^2 exp(x_i^u+y_j^u)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------------------#\n#                     Standard imports                         #\n#--------------------------------------------------------------#\nimport time\nimport torch\nfrom torch.autograd import grad\n\n\n#--------------------------------------------------------------#\n#   Please use the \"verbose\" compilation mode for debugging    #\n#--------------------------------------------------------------#\n#import sys, os.path\n#sys.path.append(os.path.dirname(os.path.abspath(__file__)) + (os.path.sep + '..')*2)\n\nfrom pykeops.torch import Genred\n\n# import pykeops\n# pykeops.verbose = False\n\n#--------------------------------------------------------------#\n#                   Define our dataset                         #\n#--------------------------------------------------------------#\nM = 3000\nN = 5000\n\ntype = 'float32' # Could be 'float32' or 'float64'\ntorchtype = torch.float32 if type == 'float32' else torch.float64\n\nx = torch.randn(M, 3, dtype=torchtype)\ny = torch.randn(N, 3, dtype=torchtype, requires_grad=True)\na = torch.randn(N, 1, dtype=torchtype)\np = torch.randn(1, 1, dtype=torchtype)\n\n#--------------------------------------------------------------#\n#                        Kernel                                #\n#--------------------------------------------------------------#\nformula = 'Square(p-a)*Exp(x+y)'\nvariables = ['x = Vx(3)',  # First arg   : i-variable, of size 3\n             'y = Vy(3)',  # Second arg  : j-variable, of size 3\n             'a = Vy(1)',  # Third arg   : j-variable, of size 1 (scalar)\n             'p = Pm(1)']  # Fourth  arg : Parameter,  of size 1 (scalar)\n         \nstart = time.time()\n\n# The parameter reduction_op='Sum' together with axis=1 means that the reduction operation\n# is a sum over the second dimension j. Thence the results will be an i-variable.\nmy_routine = Genred(formula, variables, reduction_op='Sum', axis=1, cuda_type=type)\nc = my_routine(x, y, a, p, backend='CPU')\n\n# N.B.: By specifying backend='CPU', we make sure that the result is computed\n#       using a simple C++ for loop.\n\nprint('Time to compute the convolution operation on the cpu: ', round(time.time()-start,5), 's')\n\n#--------------------------------------------------------------#\n#                        Gradient                              #\n#--------------------------------------------------------------#\n# Now, let's compute the gradient of \"c\" with respect to y. \n# Note that since \"c\" is not scalar valued, its \"gradient\" should be understood as \n# the adjoint of the differential operator, i.e. as the linear operator that takes as input \n# a new tensor \"e\" with same size as \"c\" and outputs a tensor \"g\" with same size as \"y\"\n# such that for all variation \u03b4y of y :\n#    < dc.\u03b4y , e >_2  =  < g , \u03b4y >_2  =  < \u03b4y , dc*.e >_2\n\n# New variable of size Mx3 used as input of the gradient\ne = torch.rand_like(c)\n# Call the gradient op:\nstart = time.time()\ng = grad(c, y, e)[0]\n# PyTorch remark : grad(c, y, e) alone outputs a length 1 tuple, hence the need for [0] at the end.\n\nprint('Time to compute gradient of convolution operation on the cpu: ', round(time.time()-start,5), 's')\n\n\n\n#--------------------------------------------------------------#\n#            same operations performed on the Gpu              #\n#--------------------------------------------------------------#\n# This will of course only work if you have a Gpu...\n\nif torch.cuda.is_available():\n    # first transfer data on gpu\n    p,a,x,y,e = p.cuda(), a.cuda(), x.cuda(), y.cuda(), e.cuda()\n    # then call the operations\n    start = time.time()\n    c2 = my_routine(x, y, a, p, backend='GPU')\n    print('Time to compute convolution operation on gpu:',round(time.time()-start,5), 's ', end='')\n    print('(relative error:', float(torch.abs((c - c2.cpu()) / c).mean()), ')')\n    start = time.time()\n    g2 = grad(c2, y, e)[0]\n    print('Time to compute gradient of convolution operation on gpu:', round(time.time()-start,5), 's ', end='')\n    print('(relative error:', float(torch.abs((g - g2.cpu()) / g).mean()), ')')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}