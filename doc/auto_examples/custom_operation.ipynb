{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nCreate a new formula with KeOps\n===================================================\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# In this demo, we show how to write a (completely) new formula with KeOps.\n#\n# Using the low-level 'Sum/logsumexp/max' operators, one can compute\n# (with autodiff, without memory overflows) any formula written as :\n#\n#      a_i = Reduction_j( f( p^1,p^2,..., x^1_i,x^2_i,..., y^1_j,y^2_j,...) )\n# or   b_j = Reduction_i( f( p^1,p^2,..., x^1_i,x^2_i,..., y^1_j,y^2_j,...) )\n#\n# Where:\n# - the p^k   's are vector parameters\n# - the x^k_i 's are vector variables, indexed by 'i'\n# - the y^k_j 's are vector variables, indexed by 'j'\n# - f is an arbitrary function, defined using the './keops/core' syntax.\n# - Reduction is one of :\n#   - Sum         (Sum)\n#   - log-Sum-exp (LogSumExp)\n#   - Max         (generic_max)\n#\n#\n# In this demo file, given:\n# - p,   a vector of size 2\n# - x_i, an N-by-D array\n# - y_j, an M-by-D array\n#\n# We will compute (a_i), an N-by-D array given by:\n#\n#   a_i = sum_{j=1}^M (<x_i,y_j>**2) * ( p[0]*x_i + p[1]*y_j ) \n# \n# N.B.: if you are just interested in writing a new 'kernel' formula,\n#       you may use the (more convenient) syntax showcased in custom_kernel.py\n\n\n# Add pykeops to the path\n#import sys, os.path\n#sys.path.append(os.path.dirname(os.path.abspath(__file__)) + (os.path.sep + '..')*2)\n\n# Standard imports\nimport torch\nfrom torch.autograd import grad\nfrom pykeops.torch import Genred\n\n# Choose the storage place for our data : CPU (host) or GPU (device) memory.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef my_formula(p, x, y, backend = 'auto') :\n    \"\"\"\n    Applies a custom formula on the torch variables P, X and Y.\n    Two backends are provided, so that we can check the correctness\n    of both implementations.\n    \"\"\"\n    # Vanilla PyTorch implementation\n    if backend == 'pytorch':\n        scals = (x @ y.t())**2 # Memory-intensive computation!\n        a = p[0] * scals.sum(dim=1).view(-1,1) * x + p[1] * (scals @ y)\n        return a\n    \n    # KeOps implementation\n    else:\n        # We now expose the low-level syntax of KeOps.\n        # The library relies on vector 'Variables' which can be either:\n        # - indexed by 'i' ('x' variables, category 0)\n        # - indexed by 'j' ('y' variables, category 1)\n        # - constant across the reduction ('parameters', category 2)\n        #\n        # First of all, we must define a 'who's who' list of the variables used,\n        # by specifying their categories, index in the arguments' list, and dimensions:\n        variables = ['P = Pm(2)',                        # 1st argument,  a parameter, dim 2.\n                     'X = Vx(' + str(x.shape[1]) + ')',  # 2nd argument, indexed by i, dim D.\n                     'Y = Vy(' + str(y.shape[1]) + ')']  # 3rd argument, indexed by j, dim D.\n\n        # The actual formula:\n        # a_i   =   (<x_i,y_j>**2) * (       p[0]*x_i  +       p[1]*y_j )\n        formula = 'Pow( (X|Y) , 2) * ( (Elem(P,0) * X) + (Elem(P,1) * Y) )'\n\n        my_routine = Genred(formula, variables, reduction_op='Sum', axis=1)\n        a = my_routine(p, x, y, backend=backend)\n        return a\n\n\n# Test ========================================================================\n\n# Define our dataset\nN = 1000\nM = 2000\nD = 3\n\n# PyTorch tip: do not 'require_grad' of 'x' if you do not intend to\n#              actually compute a gradient wrt. said variable 'x'.\n#              Given this info, PyTorch (+ KeOps) is smart enough to\n#              skip the computation of unneeded gradients.\np = torch.randn(2,    requires_grad=True , device=device)\nx = torch.randn(N, D, requires_grad=False, device=device)\ny = torch.randn(M, D, requires_grad=True , device=device)\n\n# + some random gradient to backprop:\ng = torch.randn(N, D, requires_grad=True, device=device)\n\nfor backend in ['pytorch', 'auto'] :\n    print('Backend :', backend, '============================' )\n    a = my_formula(p, x, y, backend=backend)\n\n    # We can compute gradients wrt all Variables - just like with \n    # any other PyTorch operator, really.\n    # Notice the 'create_graph=True', which allows us to compute\n    # higher order derivatives if needed.\n    [grad_p, grad_y] = grad(a, [p, y], g, create_graph=True)\n\n    print('(a_i) :', a[:3,:])\n    print('(\u2202_p a).g :', grad_p )\n    print('(\u2202_y a).g :', grad_y[:3,:])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}