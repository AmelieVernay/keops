{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nBenchmark KeOps vs pytorch on convolution gradients\n===================================================\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#import sys, os.path\n#sys.path.append(os.path.dirname(os.path.abspath(__file__)) + (os.path.sep + '..')*2)\n\nimport numpy as np\nimport time, timeit\n\nfrom pykeops.numpy.utils import differences, squared_distances,grad_np_kernel, chain_rules\n\nN = 1500\nM = 300\nD = 3\nE = 3\n\ntype = 'float32'\n\n# declare numpy \n\na = np.random.rand(N, E).astype(type)\nx = np.random.rand(N, D).astype(type)\ny = np.random.rand(M, D).astype(type)\nb = np.random.rand(M, E).astype(type)\nsigma = np.array([0.4]).astype(type)\n\n# declare the torch counterpart\ntry:\n    import torch\n    \n    use_cuda = torch.cuda.is_available()\n    device = 'cuda' if use_cuda else 'cpu'\n    torchtype = torch.float32 if type == 'float32' else torch.float64\n\n    ac = torch.tensor(a, dtype=torchtype, device=device)\n    xc = torch.tensor(x, dtype=torchtype, device=device, requires_grad=True)\n    yc = torch.tensor(y, dtype=torchtype, device=device)\n    bc = torch.tensor(b, dtype=torchtype, device=device)\n    sigmac = torch.tensor(sigma, dtype=torchtype, device=device)\n\nexcept:\n    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark\n###########################################################\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "enable_GC = False # Garbage collection?\nGC = 'gc.enable();' if enable_GC else 'pass;'\nLOOPS = 200\nprint('Times to compute ', LOOPS, ' grad-convolutions of size {}x{}:'.format(M, N), end='\\n')\n\n\nfor k in (['gaussian', 'laplacian', 'cauchy', 'inverse_multiquadric']):\n    print('----------------------------------- kernel: ' + k)\n\n    # pure numpy\n    gnumpy = chain_rules(a, x, y, grad_np_kernel(x, y, sigma, kernel=k), b)\n    speed_numpy = timeit.Timer('gnumpy = chain_rules(a, x, y, grad_np_kernel(x, y, sigma, kernel=k), b)',\n                               GC, globals = globals(),\n                               timer = time.time).timeit(LOOPS)\n    print('Time for numpy:            {:.4f}s'.format(speed_numpy))\n\n    # keops + pytorch : generic tiled implementation (with cuda if available else uses cpu)\n    try:\n        from pykeops.torch import Kernel, kernel_product\n    \n        params = {\n            'id': Kernel(k + '(x,y)'),\n            'gamma': 1. / (sigmac * sigmac),\n            'backend': 'auto',\n        }\n    \n        aKxy_b = torch.dot(ac.view(-1), kernel_product(params, xc, yc, bc, mode='sum').view(-1))\n        g3 = torch.autograd.grad(aKxy_b, xc, create_graph=False)[0].cpu()\n        speed_keops = timeit.Timer(\"g3 = torch.autograd.grad(torch.dot(ac.view(-1), kernel_product(params, xc, yc, bc, mode='sum').view(-1)), xc, create_graph=False)[0]\",\n                                   GC, globals = globals(),\n                                   timer = time.time).timeit(LOOPS)\n        print('Time for Keops+pytorch:    {:.4f}s'.format(speed_keops),end='')\n        print('   (absolute error:       ', np.max(np.abs(g3.data.numpy() - gnumpy)), ')')\n    except:\n        print('Time for keops generic:       Not Done')\n    \n    # vanilla pytorch (with cuda if available else uses cpu)\n    try:\n        from pykeops.torch import Kernel, kernel_product\n\n        params = {\n            'id': Kernel(k + '(x,y)'),\n            'gamma': 1. / (sigmac * sigmac),\n            'backend': 'pytorch',\n        }\n\n        aKxy_b = torch.dot(ac.view(-1), kernel_product(params, xc, yc, bc, mode='sum').view(-1))\n        g3 = torch.autograd.grad(aKxy_b, xc, create_graph=False)[0].cpu()\n        speed_keops = timeit.Timer(\"g3 = torch.autograd.grad(torch.dot(ac.view(-1), kernel_product(params, xc, yc, bc, mode='sum').view(-1)), xc, create_graph=False)[0]\",\n                                   GC, globals = globals(),\n                                   timer = time.time).timeit(LOOPS)\n        print('Time for Pytorch:          {:.4f}s'.format(speed_keops),end='')\n        print('   (absolute error:       ', np.max(np.abs(g3.data.numpy() - gnumpy)), ')')\n    except:\n        print('Time for Pytorch:             Not Done')\n        \n    # specific cuda tiled implementation (if cuda is available)\n    try:\n        from pykeops.numpy import RadialKernelGrad1conv\n        my_conv = RadialKernelGrad1conv(type)\n        g1 = my_conv(a, x, y, b, sigma, kernel=k)\n        \n        speed_pykeops = timeit.Timer('g1 = my_conv(a, x, y, b, sigma, kernel=k)',\n                                     GC, globals = globals(),\n                                     timer = time.time).timeit(LOOPS)\n        print('Time for keops specific:   {:.4f}s'.format(speed_pykeops),end=\"\")\n        print('   (absolute error:       ', np.max(np.abs (g1 - gnumpy)), ')')\n    except:\n        print('Time for keops cuda specific: Not Done')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}