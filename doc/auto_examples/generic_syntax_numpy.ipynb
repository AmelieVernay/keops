{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nExample of KeOps reduction using the generic syntax. \n=====================================================\n\nThis example uses a pure numpy framework (without Pytorch).\n\n# this example computes the following tensor operation :\n# inputs :\n#   x   : a 3000x3 tensor, with entries denoted x_i^u\n#   y   : a 5000x3 tensor, with entries denoted y_j^u\n#   a   : a 5000x1 tensor, with entries denoted a_j\n#   p   : a scalar (entered as a 1x1 tensor)\n# output :\n#   c   : a 3000x3 tensor, with entries denoted c_i^u, such that\n#   c_i^u = sum_j (p-a_j)^2 exp(x_i^u+y_j^u)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------------------#\n#                     Standard imports                         #\n#--------------------------------------------------------------#\nimport time\nimport numpy as np\n\n\n#--------------------------------------------------------------#\n#   Please use the \"verbose\" compilation mode for debugging    #\n#--------------------------------------------------------------#\n#import sys, os.path\n#sys.path.append(os.path.dirname(os.path.abspath(__file__)) + (os.path.sep + '..')*2)\n\nfrom pykeops.numpy import Genred\n\n# import pykeops\n# pykeops.verbose = False\n\n\n#--------------------------------------------------------------#\n#                   Define our dataset                         #\n#--------------------------------------------------------------#\nM = 3000\nN = 5000\n\ntype = 'float32'  # May be 'float32' or 'float64'\n\nx = np.random.randn(M,3).astype(type)\ny = np.random.randn(N,3).astype(type)\na = np.random.randn(N,1).astype(type)\np = np.random.randn(1,1).astype(type)\n\n\n#--------------------------------------------------------------#\n#                        Kernel                                #\n#--------------------------------------------------------------#\nformula = 'Square(p-a)*Exp(x+y)'\nvariables = ['x = Vx(3)',  # First arg   : i-variable, of size 3\n             'y = Vy(3)',  # Second arg  : j-variable, of size 3\n             'a = Vy(1)',  # Third arg   : j-variable, of size 1 (scalar)\n             'p = Pm(1)']  # Fourth  arg : Parameter,  of size 1 (scalar)\n\n\nstart = time.time()\n\n# The parameter reduction_op='Sum' together with axis=1 means that the reduction operation\n# is a sum over the second dimension j. Thence the results will be an i-variable.\nmy_routine = Genred(formula, variables, reduction_op='Sum', axis=1, cuda_type=type)\nc = my_routine(x, y, a, p, backend=\"auto\")\n\n# N.B.: If CUDA is available + backend=\"auto\" (or not specified),\n#       KeOps will load the data on the GPU + compute + unload the result back to the CPU,\n#       as it is assumed to be more efficient.\n#       By specifying backend=\"CPU\", you can make sure that the result is computed\n#       using a simple C++ for loop\n\nprint(\"Time to compute the convolution operation: \",round(time.time()-start,5),\"s\")\n\n#--------------------------------------------------------------#\n#                        Gradient                              #\n#--------------------------------------------------------------#\n# Now, let's compute the gradient of \"c\" with respect to y. \n# Note that since \"c\" is not scalar valued, its \"gradient\" should be understood as \n# the adjoint of the differential operator, i.e. as the linear operator that takes as input \n# a new tensor \"e\" with same size as \"c\" and outputs a tensor \"g\" with same size as \"y\"\n# such that for all variation \u03b4y of y :\n#    < dc.\u03b4y , e >_2  =  < g , \u03b4y >_2  =  < \u03b4y , dc*.e >_2\n\n# New variable of size Mx3 used as input of the gradient\ne = np.random.randn(M, 3).astype(type)\n\n# Thankfully, KeOps provides an autodiff engine for formulas. However, without PyTorch's\n# autodiff engine, we need to specify everything by hand: simply add the gradient operator\n#  around formula: i.e  Grad( the_formula , variable_to_differentiate, input_of_the_gradient)\nformula_grad =  'Grad(' + formula + ', y, e)'\n\n# This new formula has an a new variable (namely the input variable e)\nvariables_grad = variables + ['e = Vx(3)'] # Fifth  arg : i-variable, of size 3 ... Just like \"c\" !\n\nmy_grad = Genred(formula_grad, variables_grad, reduction_op='Sum', axis=1, cuda_type=type)\n\nstart = time.time()\nd = my_grad(x, y, a, p, e)\nprint('Time to compute the gradient of the convolution operation: ', round(time.time()-start,5), 's')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}