
///////////////////////////////////////
/////////// CUDA KERNEL ///////////////
///////////////////////////////////////

template < typename TYPE, int DIMPOINT, int DIMVECT, int TILE_SIZE, KernelFun KernelF >
__global__ void KernelGpuTileOnDevice(TYPE ooSigma2,
                                       TYPE *x, TYPE *y, TYPE *beta, TYPE *gamma,
                                       int nx, int ny) {
    // Thread kernel:
    // Computation of gamma_i = sum_j k(x_i,y_j)*beta_j
    // for index i given by thread id.

    int i = blockIdx.x * blockDim.x + threadIdx.x;
 
    const int rect = blockDim.x / TILE_SIZE;
    const int tileDim = TILE_SIZE;//blockDim.x / rect;

    int threadId_tile = threadIdx.x %  tileDim;
    int tileIdx = threadIdx.x / tileDim;
    //printf("toto %d\n",tileDim);

    extern __shared__ TYPE SharedData[];  // shared data will contain x and alpha data for the block

    // One thread = One line = One x_i + One gamma_i + a whole bunch of "y_j".
    TYPE xi[DIMPOINT], gammai[DIMVECT];
    if(i<nx) { // we will compute gammai only if i is in the range
        for(int k=0; k<DIMPOINT; k++)
            xi[k] = x[i*DIMPOINT+k];  // Load xi from device global memory
        for(int k=0; k<DIMVECT; k++)
            gammai[k] = 0.0f;         // Make sure to put to zero the output array
    }




    // We use all the threads to load the tiles in the memory :
    for(int jstart = 0, tile = 0; jstart < ny; jstart += TILE_SIZE, tile++) {

        // Load data in Shared memory -----------------------------------------------------------


        int j = tile * TILE_SIZE + threadId_tile; // Current column
        // We load yj and betaj from device global memory...
        if(j<ny) { // ...only if j<ny (we may be in the last columns of the last tile...)
            // Pretty uneasy to read : we store yj and betaj interleaved, for better performance
            // SharedData = "[ y0, b0, y1, b1, y2, b2, ... ]"
            const int inc = DIMPOINT + DIMVECT; // Size of a  [yj, bj] block
            for(int k=tileIdx * DIMPOINT / rect; k<tileIdx * DIMPOINT / rect + DIMPOINT / rect; k++)
                SharedData[threadId_tile*inc+k]          =    y[j*DIMPOINT+k];
            for(int k=tileIdx*  DIMVECT / rect; k<tileIdx*  DIMVECT / rect + DIMVECT / rect; k++)
                SharedData[threadId_tile*inc+DIMPOINT+k] = beta[j*DIMVECT +k];

            /*printf("thread %d (%d={%d, %d}): %d to %d -- %d to %d\n", i,
                    j, threadId_tile, tileIdx,
                   tileIdx * DIMPOINT / rect, tileIdx * DIMPOINT / rect + DIMPOINT / rect,
                   tileIdx*  DIMVECT / rect, tileIdx*  DIMVECT / rect + DIMVECT / rect);
        */
             }
        __syncthreads();


        // Map-Reduction loop -------------------------------------------------------------------
        // We can now proceed to the "tiled" matrix product, where one line = one thread.
        if(i<nx) { // we compute gammai only if needed
            TYPE *yj, *betaj;              // As y_j and beta_j are interleaved...
            yj    = SharedData;            // We'll on some cute pointer arithmetics!
            betaj = SharedData + DIMPOINT;
            int inc = DIMPOINT + DIMVECT;  // The increment, size of a [y_j,b_j] block.
            for(int jrel = 0; jrel < TILE_SIZE && jrel<ny-jstart; jrel++, yj+=inc, betaj+=inc) {
                TYPE r2   = 0.0f, temp = 0.0f;
                // Compute x_i-y_j and its squared norm:
                for(int k=0; k<DIMPOINT; k++) {
                    temp    =  xi[k]-yj[k];
                    r2     +=   temp*temp;
                }
                // Straighforward inplace reduction loop over j : at last, we're getting to the maths... **********
                TYPE s = KernelF(r2,ooSigma2);  // The kernel function is stored in "radial_kernels.cx"
                for(int k=0; k<DIMVECT; k++) {  // Add the vector s*beta_j to gamma_i
                    gammai[k] += s * betaj[k]; // (no need to be extra-clever here)
                }
                // ************************************************************************************************
            }
        }

        // Once the loop is over, the current tiled matrix product has been reduced to gamma_i
        __syncthreads(); // So make sure that no one's left behind...
        // And move on to the next tile.
    }

    // Save the result in global memory.
    if(i<nx)
        for(int k=0; k<DIMVECT; k++) {
            gamma[i * DIMVECT + k] = gammai[k];
            //printf("gammai = %g\n", gammai[k]);
        }
}



//////////////////////////////////////////////////////
/////////// CPU -> GPU -> CPU routines ///////////////
//////////////////////////////////////////////////////

template < typename TYPE, KernelFun KernelF, int TOTO_DIM, int BLOCK_SIZE, int TILE_SIZE >
int KernelGpuEvalTile(TYPE ooSigma2,
                      TYPE* x_h, TYPE* y_h, TYPE* beta_h, TYPE* gamma_h,
                      int dimPoint, int dimVect, int nx, int ny) {

    // Data on the device.
    TYPE* x_d;
    TYPE* y_d;
    TYPE* beta_d;
    TYPE* gamma_d;

    // Allocate arrays on device.
    cudaMalloc((void**)&x_d,     sizeof(TYPE)*(nx*dimPoint));
    cudaMalloc((void**)&y_d,     sizeof(TYPE)*(ny*dimPoint));
    cudaMalloc((void**)&beta_d,  sizeof(TYPE)*(ny*dimVect ));
    cudaMalloc((void**)&gamma_d, sizeof(TYPE)*(nx*dimVect ));

    // Set values to zeros
    cudaMemset(x_d,    0, sizeof(TYPE)*(nx*dimPoint));
    cudaMemset(y_d,    0, sizeof(TYPE)*(ny*dimPoint));
    cudaMemset(beta_d, 0, sizeof(TYPE)*(ny*dimVect ));
    cudaMemset(gamma_d,0, sizeof(TYPE)*(nx*dimVect ));

    // Send data from host to device.
    cudaMemcpy(x_d,    x_h,    sizeof(TYPE)*(nx*dimPoint), cudaMemcpyHostToDevice);
    cudaMemcpy(y_d,    y_h,    sizeof(TYPE)*(ny*dimPoint), cudaMemcpyHostToDevice);
    cudaMemcpy(beta_d, beta_h, sizeof(TYPE)*(ny*dimVect ), cudaMemcpyHostToDevice);

    // Compute on device.
    dim3 blockSize;
    blockSize.x = BLOCK_SIZE; // number of threads in each block
    dim3 gridSize;
    gridSize.x =  nx / blockSize.x + (nx%blockSize.x==0 ? 0 : 1);

    // Copy-paste templating, allowing us to pass the DIMPOINT and DIMVECT at compilation time :
    if(dimPoint==TOTO_DIM && dimVect==TOTO_DIM)
        KernelGpuTileOnDevice<TYPE, TOTO_DIM, TOTO_DIM, TILE_SIZE, KernelF><<<gridSize,blockSize,TILE_SIZE *(dimVect+dimPoint)*sizeof(TYPE)>>>
                (ooSigma2, x_d, y_d, beta_d, gamma_d, nx, ny);

    else {
        printf("Error: dimensions of Gauss kernel not implemented in cuda. You probably just need a copy-paste in the conda_tile.cu file !");
        cudaFree(x_d);
        cudaFree(y_d);
        cudaFree(beta_d);
        cudaFree(gamma_d);
        return(-1);
    }

    // block until the device has completed
    cudaDeviceSynchronize();

    // Send data from device to host.
    cudaMemcpy(gamma_h, gamma_d, sizeof(TYPE)*(nx*dimVect),cudaMemcpyDeviceToHost);

    // Free memory.
    cudaFree(x_d);
    cudaFree(y_d);
    cudaFree(beta_d);
    cudaFree(gamma_d);

    return 0;
}
