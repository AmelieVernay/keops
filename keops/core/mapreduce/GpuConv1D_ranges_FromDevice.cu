#include <stdio.h>
#include <iostream>
#include <assert.h>
#include <cuda.h>

#include "core/pack/Pack.h"
#include "core/pack/GetInds.h"
#include "core/pack/GetDims.h"
#include "core/mapreduce/broadcast_batch_dimensions.h"
#include "core/utils/CudaErrorCheck.cu"
#include "core/mapreduce/GpuConv1D_ranges.h"

namespace keops {

struct GpuConv1D_ranges_FromDevice {
  template < typename TYPE, class FUN >
  static int Eval_(FUN fun, int nx, int ny,
                   int nbatchdims, int *shapes,
                   int nranges_x, int nranges_y, __INDEX__ **ranges,
                   TYPE** phx_d, TYPE** phy_d, TYPE** php_d) {
    
    typedef typename FUN::DIMSX DIMSX;
    typedef typename FUN::DIMSY DIMSY;
    typedef typename FUN::DIMSP DIMSP;
    const int DIMY = DIMSY::SUM;
    const int SIZEI = DIMSX::SIZE;
    const int SIZEJ = DIMSY::SIZE;
    const int SIZEP = DIMSP::SIZE;
    
    // device arrays of pointers to device data
    TYPE **px_d, **py_d, **pp_d;
    
    // single cudaMalloc
    void **p_data;
    CudaSafeCall(cudaMalloc((void**)&p_data, sizeof(TYPE*)*(SIZEI+SIZEJ+SIZEP)));
    
    TYPE **p_data_a = (TYPE**)p_data;
    px_d = p_data_a;
    p_data_a += SIZEI;
    py_d = p_data_a;
    p_data_a += SIZEJ;
    pp_d = p_data_a;
    
    CudaSafeCall(cudaMemcpy(px_d, phx_d, SIZEI*sizeof(TYPE*), cudaMemcpyHostToDevice));
    CudaSafeCall(cudaMemcpy(py_d, phy_d, SIZEJ*sizeof(TYPE*), cudaMemcpyHostToDevice));
    CudaSafeCall(cudaMemcpy(pp_d, php_d, SIZEP*sizeof(TYPE*), cudaMemcpyHostToDevice));
    
    // Compute on device : grid and block are both 1d
    
    cudaDeviceProp deviceProp;
    int dev = -1;
    CudaSafeCall(cudaGetDevice(&dev));
    CudaSafeCall(cudaGetDeviceProperties(&deviceProp, dev));
    
    dim3 blockSize;
    // warning : blockSize.x was previously set to CUDA_BLOCK_SIZE; currently CUDA_BLOCK_SIZE value is used as a bound.
    blockSize.x = ::std::min(CUDA_BLOCK_SIZE,::std::min(deviceProp.maxThreadsPerBlock, (int) (deviceProp.sharedMemPerBlock / ::std::max(1, (int)(DIMY*sizeof(TYPE))) ))); // number of threads in each block
    
    
    // Ranges pre-processing... ==================================================================
    
    // N.B.: In the following code, we assume that the x-ranges do not overlap.
    //       Otherwise, we'd have to assume that DIMRED == DIMOUT
    //       or allocate a buffer of size nx * DIMRED. This may be done in the future.
    // Cf. reduction.h:
    //    FUN::tagJ = 1 for a reduction over j, result indexed by i
    //    FUN::tagJ = 0 for a reduction over i, result indexed by j
    
    int nranges = FUN::tagJ ? nranges_x : nranges_y ;
    __INDEX__ *ranges_x = FUN::tagJ ? ranges[0] : ranges[3] ;
    __INDEX__ *slices_x = FUN::tagJ ? ranges[1] : ranges[4] ;
    __INDEX__ *ranges_y = FUN::tagJ ? ranges[2] : ranges[5] ;
    
    __INDEX__ *ranges_x_h = NULL, *slices_x_d = NULL, *ranges_y_d = NULL;
    
    // The code below needs a pointer to ranges_x on *host* memory,  -------------------
    // as well as pointers to slices_x and ranges_y on *device* memory.
    // -> Depending on the "ranges" location, we'll copy ranges_x *or* slices_x and ranges_y
    //    to the appropriate memory:
    bool ranges_on_device = (nbatchdims==0);
    // N.B.: We only support Host ranges with Device data when these ranges were created
    //       to emulate block-sparse reductions.
    
    if ( ranges_on_device ) {  // The ranges are on the device
      ranges_x_h = new __INDEX__[2*nranges] ;
      // Send data from device to host.
      CudaSafeCall(cudaMemcpy(ranges_x_h, ranges_x, sizeof(__INDEX__)*2*nranges, cudaMemcpyDeviceToHost));
      slices_x_d = slices_x;
      ranges_y_d = ranges_y;
    }
    else {  // The ranges are on host memory; this is typically what happens with **batch processing**,
      // with ranges generated by keops_io.h:
      ranges_x_h = ranges_x;
      
      // Copy "slices_x" to the device:
      CudaSafeCall(cudaMalloc((__INDEX__**)&slices_x_d, sizeof(__INDEX__)*nranges));
      CudaSafeCall(cudaMemcpy(slices_x_d, slices_x, sizeof(__INDEX__)*nranges, cudaMemcpyHostToDevice));
      
      // Copy "redranges_y" to the device: with batch processing, we KNOW that they have the same shape as ranges_x
      CudaSafeCall(cudaMalloc((__INDEX__**)&ranges_y_d, sizeof(__INDEX__)*2*nranges));
      CudaSafeCall(cudaMemcpy(ranges_y_d, ranges_y, sizeof(__INDEX__)*2*nranges, cudaMemcpyHostToDevice));
    }
    
    // Computes the number of blocks needed ---------------------------------------------
    int nblocks = 0, len_range = 0;
    for(int i=0; i<nranges ; i++){
      len_range = ranges_x_h[2*i+1] - ranges_x_h[2*i] ;
      nblocks += (len_range/blockSize.x) + (len_range%blockSize.x==0 ? 0 : 1) ;
    }
    
    // Create a lookup table for the blocks --------------------------------------------
    __INDEX__ *lookup_h = NULL;
    lookup_h = new __INDEX__[3*nblocks] ;
    int index = 0;
    for(int i=0; i<nranges ; i++){
      len_range = ranges_x_h[2*i+1] - ranges_x_h[2*i] ;
      for(int j=0; j<len_range ; j+=blockSize.x) {
        lookup_h[3*index]   = i;
        lookup_h[3*index+1] = ranges_x_h[2*i] + j;
        lookup_h[3*index+2] = ranges_x_h[2*i] + j + ::std::min((int) blockSize.x, len_range-j ) ;
        
        index++;
      }
    }
    
    // Load the table on the device -----------------------------------------------------
    __INDEX__ *lookup_d = NULL;
    CudaSafeCall(cudaMalloc((__INDEX__**)&lookup_d, sizeof(__INDEX__)*3*nblocks));
    CudaSafeCall(cudaMemcpy(lookup_d, lookup_h, sizeof(__INDEX__)*3*nblocks, cudaMemcpyHostToDevice));
    
    // Support for broadcasting over batch dimensions =============================================
    
    // We create a lookup table, "offsets", of shape (nblock, SIZEVARS):
    int *offsets_d = NULL;
    
    if (nbatchdims > 0) {
      offsets_d = build_offset_tables<FUN>( nbatchdims, shapes, nblocks, lookup_h );
    }
    
    // ============================================================================================
    
    dim3 gridSize;
    gridSize.x =  nblocks ; //nx / blockSize.x + (nx%blockSize.x==0 ? 0 : 1);
    
    // Size of the SharedData : blockSize.x*(DIMY)*sizeof(TYPE)
    GpuConv1DOnDevice_ranges<TYPE><<<gridSize,blockSize,blockSize.x*(DIMY)*sizeof(TYPE)>>>(fun,nx,ny,
            nbatchdims,shapes, offsets_d,
            lookup_d,slices_x_d,ranges_y_d,
            px_d,py_d,pp_d);
    
    // block until the device has completed
    CudaSafeCall(cudaDeviceSynchronize());
    CudaCheckError();
    
    CudaSafeCall(cudaFree(p_data));
    
    // Free the block lookup table :
    delete [] lookup_h;
    CudaSafeCall(cudaFree(lookup_d));
    
    // Free the host or device "ranges" copies:
    if (ranges_on_device) {
      delete [] ranges_x_h;
    } else {
      CudaSafeCall(cudaFree(slices_x_d));
      CudaSafeCall(cudaFree(ranges_y_d));
    }
    
    if (nbatchdims > 0) {
      CudaSafeCall(cudaFree(offsets_d));
    }
    
    
    return 0;
  }

// Same wrappers, but for data located on the device
  template < typename TYPE, class FUN, typename... Args >
  static int Eval(FUN fun, int nx, int ny,
                  int nbatchdims, int *shapes,
                  int nranges_x, int nranges_y, __INDEX__ **ranges,
                  int device_id, TYPE* x1_d, Args... args) {
    
    // device_id is provided, so we set the GPU device accordingly
    // Warning : is has to be consistent with location of data
    CudaSafeCall(cudaSetDevice(device_id));
    
    typedef typename FUN::VARSI VARSI;
    typedef typename FUN::VARSJ VARSJ;
    typedef typename FUN::VARSP VARSP;
    
    const int SIZEI = VARSI::SIZE+1;
    const int SIZEJ = VARSJ::SIZE;
    const int SIZEP = VARSP::SIZE;
    
    using DIMSX = GetDims<VARSI>;
    using DIMSY = GetDims<VARSJ>;
    using DIMSP = GetDims<VARSP>;
    
    using INDSI = GetInds<VARSI>;
    using INDSJ = GetInds<VARSJ>;
    using INDSP = GetInds<VARSP>;
    
    TYPE *phx_d[SIZEI];
    TYPE *phy_d[SIZEJ];
    TYPE *php_d[SIZEP];
    
    phx_d[0] = x1_d;
    
    getlist<INDSI>(phx_d+1,args...);
    getlist<INDSJ>(phy_d,args...);
    getlist<INDSP>(php_d,args...);
    
    return Eval_(fun,nx,ny,nbatchdims,shapes,nranges_x,nranges_y,ranges,phx_d,phy_d,php_d);
    
  }

// same without the device_id argument
  template < typename TYPE, class FUN, typename... Args >
  static int Eval(FUN fun, int nx, int ny,
                  int nbatchdims, int *shapes,
                  int nranges_x, int nranges_y, __INDEX__ **ranges,
                  TYPE* x1_d, Args... args) {
    // We set the GPU device on which computations will be performed
    // to be the GPU on which data is located.
    // NB. we only check location of x1_d which is the output vector
    // so we assume that input data is on the same GPU
    // note : cudaPointerGetAttributes has a strange behaviour:
    // it looks like it makes a copy of the vector on the default GPU device (0) !!!
    // So we prefer to avoid this and provide directly the device_id as input (first function above)
    cudaPointerAttributes attributes;
    CudaSafeCall(cudaPointerGetAttributes(&attributes,x1_d));
    return Eval(fun, nx, ny, nbatchdims, shapes, nranges_x,nranges_y,ranges, attributes.device, x1_d, args...);
  }
  
  template < typename TYPE, class FUN >
  static int Eval(FUN fun, int nx, int ny,
                  int nbatchdims, int *shapes,
                  int nranges_x, int nranges_y, __INDEX__ **ranges,
                  TYPE* x1_d, TYPE** args, int device_id=-1) {
    
    if(device_id==-1) {
      // We set the GPU device on which computations will be performed
      // to be the GPU on which data is located.
      // NB. we only check location of x1_d which is the output vector
      // so we assume that input data is on the same GPU
      // note : cudaPointerGetAttributes has a strange behaviour:
      // it looks like it makes a copy of the vector on the default GPU device (0) !!!
      // So we prefer to avoid this and provide directly the device_id as input (else statement below)
      cudaPointerAttributes attributes;
      CudaSafeCall(cudaPointerGetAttributes(&attributes,x1_d));
      CudaSafeCall(cudaSetDevice(attributes.device));
    } else // device_id is provided, so we use it. Warning : is has to be consistent with location of data
      CudaSafeCall(cudaSetDevice(device_id));
    
    typedef typename FUN::VARSI VARSI;
    typedef typename FUN::VARSJ VARSJ;
    typedef typename FUN::VARSP VARSP;
    
    const int SIZEI = VARSI::SIZE+1;
    const int SIZEJ = VARSJ::SIZE;
    const int SIZEP = VARSP::SIZE;
    
    using DIMSX = GetDims<VARSI>;
    using DIMSY = GetDims<VARSJ>;
    using DIMSP = GetDims<VARSP>;
    
    using INDSI = GetInds<VARSI>;
    using INDSJ = GetInds<VARSJ>;
    using INDSP = GetInds<VARSP>;
    
    TYPE *px_d[SIZEI];
    TYPE *py_d[SIZEJ];
    TYPE *pp_d[SIZEP];
    
    px_d[0] = x1_d;
    for(int i=1; i<SIZEI; i++)
      px_d[i] = args[INDSI::VAL(i-1)];
    for(int i=0; i<SIZEJ; i++)
      py_d[i] = args[INDSJ::VAL(i)];
    for(int i=0; i<SIZEP; i++)
      pp_d[i] = args[INDSP::VAL(i)];
    
    return Eval_(fun,nx,ny,nbatchdims,shapes,nranges_x,nranges_y,ranges,px_d,py_d,pp_d);
    
  }
  
};

}


using namespace keops;

extern "C" int GpuReduc1D_ranges_FromDevice(int nx, int ny,
                                            int nbatchdims, int *shapes,
                                            int nranges_x, int nranges_y, __INDEX__ **castedranges,
                                            __TYPE__ *gamma, __TYPE__ **args, int device_id = -1) {
  return Eval< F, GpuConv1D_ranges_FromDevice >::Run(nx,
                                                     ny,
                                                     nbatchdims,
                                                     shapes,
                                                     nranges_x,
                                                     nranges_y,
                                                     castedranges,
                                                     gamma,
                                                     args,
                                                     device_id);
}
