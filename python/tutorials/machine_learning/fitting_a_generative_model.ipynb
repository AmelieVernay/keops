{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a generative model using standard divergences between measures\n",
    "\n",
    "In this notebook, we show how to tune the parameters of a generative model to match it with an empirical distribution.\n",
    "Having observed a sample $y_1, \\dots, y_M \\in \\mathbb{R}^d$, we assume that the dataset was drawn according to an i.i.d. process of law $\\mu_{\\text{gen}} \\in \\mathbb{P}(\\mathbb{R}^d)$ which models an unknown generative process.\n",
    "We represent the **target** empirical distribution as a probability measure\n",
    "$$ \\nu = \\frac{1}{M} \\sum_j \\delta_{y_j} = \\sum_j \\nu_j \\, \\delta_{y_j}.$$\n",
    "Generically, the training dataset is thus modeled as a **sum of weighted dirac masses** which samples an unknown probability measure $\\mu_{\\text{gen}}$.\n",
    "\n",
    "**The density fitting problem.** Given this input distribution $\\nu$, we wish to extract its main features and, if possible, mimick it by generating \"plausible\" vectors $x_1, \\dots, x_N \\in \\mathbb{R^d}$; regressing $\\mu_{\\text{gen}}$ in an intelligent way. An obvious solution would be to draw samples from the finite set $\\{y_1, \\dots, y_M\\}$ with weights given by the $\\nu_j$'s...\n",
    "But this is not very satisfying. Intuitively, if the $y_j$'s were drawn according to a Gaussian distribution, we should be able to regress its mean and covariance matrix; not restrict ourselves to a finite sampling!\n",
    "Hence, **to give a relevant answer to a density estimation problem, we must introduce some prior**; assert that some probability measures (say, Gaussian laws) are much more likely to be \"the real underlying distribution $\\mu_{\\text{gen}}$\" than finite probability measures on some randomish point cloud.\n",
    "\n",
    "**An explicit cost to minimize.** Today, in line with a ``PyTorch``-friendly algorithmic paradigm, we choose to encode our prior in **the explicit structure of a generating program**.\n",
    "A deterministic application $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^d$ (i.e. a vector-valued program) defines a natural pushforward action \n",
    "\n",
    "$$ f\\# : \\mathbb{P}(\\mathbb{R}^n) \\rightarrow \\mathbb{P}(\\mathbb{R}^d) $$\n",
    "\n",
    "such that sampling $f\\#\\mu_0$ is equivalent to applying $f$ to $\\mu_0$-samples.\n",
    "Hence, if $\\mu_0$ is a (simple) reference measure in $\\mathbb{R}^n$, and if $(f_w)_{w\\in \\mathbb{R}^p}$\n",
    "is a family of deterministic functions mapping $\\mathbb{R}^n$ to $\\mathbb{R}^d$\n",
    "parametrized by vectors $w\\in \\mathbb{R}^p$, we can **look for model densities $\\mu_w$ of the form**\n",
    "\n",
    "$$ \\mu_w ~=~ f_w\\# \\mu_0  \\in \\mathbb{P}(\\mathbb{R}^d), ~~ \\text{which should be close to $\\mu_{\\text{gen}}$.}$$\n",
    "\n",
    "This way, we restrict the search space to the model measures *that can be desribed using $f_w$*.\n",
    "To formulate a well-posed problem, all we need now is **a discrepancy (data attachment) formula $\\text{d}$,\n",
    "which quantifies how far away the model distribution $f_w \\# \\mu_0$ is to the target $\\nu$**.\n",
    "Assuming we have such a convenient formula at hand,\n",
    "we can then look for the optimal vector of parameters $w\\in \\mathbb{R}^p$ that minimizes the real-valued cost\n",
    "\n",
    "$$ \\text{Cost}(w) ~=~ \\text{d}( \\mu_w, \\nu ).$$\n",
    "\n",
    "Hopefully, a gradient-descent like \n",
    "scheme can converge towards a good-enough value $w^*$, and we use $\\mu_{w^*} = f_{w^*} \\# \\mu_0$\n",
    "as an estimate for the underlying distribution $\\mu_{\\text{gen}}$.\n",
    "\n",
    "**Plan of this notebook.** The procedure described above can help us to *regress densities* by enforcing a strong generative prior... But we still have to work out the details! In the first part, we introduce the `PyTorch` syntax needed to encode a simple (polynomial) generative model for measures on $\\mathbb{R}^2$. Then, we show how to implement three standard divergences $\\text{d}(\\mu_w, \\nu)$ between sampled measures:\n",
    "- The kernel distance $\\text{d}_k$, linked to the theory of Reproducing Kernel Hilbert Spaces.\n",
    "- The log-likelihood score $\\text{d}_{\\text{ML}}$, related to the Kullback-Leibler divergence of information theory.\n",
    "- The Sinkhorn distance $\\text{d}_{\\text{OT}}$, defined through a relaxation of Optimal Transport theory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the standard array-related libraries (MATLAB-like)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from   matplotlib.collections  import LineCollection\n",
    "%matplotlib nbagg\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the automatic differentiation + GPU toolbox\n",
    "import torch\n",
    "from torch          import nn\n",
    "from torch.nn       import Parameter\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shall we use the GPU?\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dtype    = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "NPOINTS  = 200 if use_cuda else 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A computational building block: the kernel product\n",
    "\n",
    "Most standard discrepancies $\\text{d}$ between sampled measures can be computed using a **kernel product operator**\n",
    "\n",
    "$$ \\text{KP} :~ \\big((x_i), (y_j), (\\nu_j)\\big) \\in \\mathbb{R}^{N\\cdot d}\\times \\mathbb{R}^{M\\cdot d} \\times \\mathbb{R}^{M\\cdot 1} ~~ \\mapsto ~~ \\bigg( \\sum_j k(x_i-y_j)\\,\\nu_j \\bigg)_i \\in \\mathbb{R}^{N\\cdot 1}$$\n",
    "\n",
    "where $k:\\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a convolution kernel. Mathematically, this operation is known as a **discrete convolution**:\n",
    "Indeed, if $\\nu = \\sum_j \\nu_j \\delta_{y_j}$ is a discrete measure, the convolution\n",
    "product $k\\star \\nu$ is a function defined on $\\mathbb{R}^d$ by\n",
    "\n",
    "$$\\big(k\\star\\nu \\big)(x) ~=~ \\sum_j k(x-y_j) \\,\\nu_j,$$\n",
    "\n",
    "so that computing the kernel product $\\text{KP}\\big((x_i), (y_j), (\\nu_j)\\big)$\n",
    "is equivalent to computing and sampling $k\\star \\nu$ on the point cloud $(x_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "backend = \"pytorch\"\n",
    "\n",
    "if backend == \"pytorch\" :\n",
    "    def kernel_product(x,y,nu, mode = \"gaussian\", s = 1.) :\n",
    "        \"\"\"\n",
    "        Computes K(x_i,y_j) @ nu_j = \\sum_j k(x_i-y_j) * nu_j\n",
    "        where k is a kernel function (say, a Gaussian) of deviation s.\n",
    "        \"\"\"\n",
    "        x_i = x.unsqueeze(1)        # Shape (N,d) -> Shape (N,1,d)\n",
    "        y_j = y.unsqueeze(0)        # Shape (M,d) -> Shape (1,M,d)\n",
    "        xmy = ((x_i-y_j)**2).sum(2) # xmy[i,j] = |x_i-y_j|^2\n",
    "        if   mode == \"gaussian\" : K = torch.exp( - xmy / (s**2) )\n",
    "        elif mode == \"laplace\"  : K = torch.exp( - torch.sqrt(xmy + (s**2)) )\n",
    "        elif mode == \"energy\"   : K = torch.pow(   xmy + (s**2), -.25 )\n",
    "        return K @ (nu.view(-1,1))\n",
    "elif backend == \"libkp\" :\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A GMM-polynomial generative model\n",
    "\n",
    "Using `PyTorch` (or any other automatic differentiation library), defining generative models is very easy: from mixture models to ODE flows, if you can define it, you can implement it. To keep things simple in this demo notebook, we define a **polynomial Gaussian Mixture Model** as follow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PolynomialMapping( coeffs, T ) :\n",
    "    \"\"\"\n",
    "    Given a set of coefficients   \"coeffs = [[x_0,y_0], [x_1,y_1], ..., [x_d,y_d]]\"\n",
    "    and a vector of \"time\" values \"   T   = [ t_0, t_1, ... ]                     \",\n",
    "    outputs a len(T)-by-2 torch variable X, such that\n",
    "    \n",
    "    X[i] = [ \\sum_{k=0}^d x_k*(t_i)^k  ,  \\sum_{k=0}^d y_k*(t_i)^k ]\n",
    "    \n",
    "    X can be thought of as the discretization at time steps T[:]\n",
    "    of the polynomial curve [ x(t), y(t) ] whose coefficients were provided by the user.\n",
    "    \"\"\"\n",
    "    X = Variable(torch.zeros( 2, len(T) ).type(dtype))\n",
    "    for (d, c) in enumerate(coeffs) :\n",
    "        X += c.view(2,1) * ( T.view(1,len(T)) )**d\n",
    "    return X.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GenerativeModel(nn.Module) :\n",
    "    \"\"\"\n",
    "    This Model generates discrete measures in the plane R^2, according to\n",
    "    the following process:\n",
    "    - a \n",
    "    \n",
    "    This class can therefore be seen as a differentiable implementation\n",
    "    of a Gaussian Mixture Model in R, embedded in the plane by a polynomial\n",
    "    mapping.\n",
    "    \n",
    "    Even though this notebook is not dedicated to \"Neural Networks\",\n",
    "    we use the convenient syntax introduced by nn.Module and nn.Parameter.\n",
    "    \"\"\"\n",
    "    def __init__(self, p = None, sigmas = None, coeffs = None, degree = 1) :\n",
    "        \"Defines the parameters of the model, using default values if necessary.\"\n",
    "        super(GenerativeModel, self).__init__()\n",
    "        if p      is None : p      = [0]\n",
    "        if sigmas is None : sigmas = [.1,.1]\n",
    "        if coeffs is None : coeffs = [[.1]*(degree+1), [.1]*(degree+1)]\n",
    "            \n",
    "        self.p      = Parameter(torch.from_numpy(np.array(p     )).type(dtype))\n",
    "        self.sigmas = Parameter(torch.from_numpy(np.array(sigmas)).type(dtype))\n",
    "        self.coeffs = Parameter(torch.from_numpy(np.array(coeffs).T).type(dtype))\n",
    "\n",
    "    def forward(self, N) :\n",
    "        \"\"\"\n",
    "        Assuming that N is an integer, generates:\n",
    "        - a batch X_i of N points in the plane - an (N,2) array\n",
    "        - a vector of weights M_i - an (N,) vector.\n",
    "        The generated measure should be understood as being equal to\n",
    "                Mu = \\sum_i M_i*\\delta_{X_i}\n",
    "        \"\"\"\n",
    "        cut = N//2\n",
    "        \n",
    "        # Sample a Gaussian distribution, and distort it to end up on\n",
    "        # the two normal laws N(-1, sigma_0), N(1, sigma_1)\n",
    "        T = Variable(torch.normal( means = torch.zeros(N) ).type(dtype) )\n",
    "        T = torch.cat( (self.sigmas[0] * T[:cut] - 1. ,\n",
    "                        self.sigmas[1] * T[cut:] + 1. ) )\n",
    "        \n",
    "        # Map the 1D coefficients to the 2D plane through a polynomial mapping\n",
    "        X = PolynomialMapping(self.coeffs, T)\n",
    "        \n",
    "        # Compute the weights associated to our diracs :\n",
    "        # overall, mass P for the first cluster and 1-P for the second one.\n",
    "        P = 1. / (1. + torch.exp(-self.p))\n",
    "        W = torch.cat( ( (   P  / cut    ) * Variable(torch.ones(  cut).type(dtype)) ,\n",
    "                         ((1-P) / (N-cut)) * Variable(torch.ones(N-cut).type(dtype)) ) )\n",
    "        return W, X\n",
    "    \n",
    "    def plot(self, axis, color = 'b') :\n",
    "        \"Displays the curve associated to the underlying polynomial mapping.\"\n",
    "        # (x(t),y(t)) for t in [-5,5]\n",
    "        t = Variable(torch.linspace(-5,5,101).type(dtype))\n",
    "        X = PolynomialMapping(self.coeffs, t)\n",
    "        X = X.data.cpu().numpy()\n",
    "        axis.plot( X[:,0], X[:,1], color+'-+', markersize = 8, linewidth=.5, zorder=-1 )\n",
    "        \n",
    "        # Puts two large dots at the \"centers\" of both sigmoids\n",
    "        t = Variable(torch.linspace(-1,1,2).type(dtype))\n",
    "        X = PolynomialMapping(self.coeffs, t)\n",
    "        X = X.data.cpu().numpy()\n",
    "        axis.scatter( X[:,0], X[:,1], 125, color, edgecolors='none' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our toy dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a model: two unbalanced classes mapped through a polynomial curve of degree = 3\n",
    "GroundTruth = GenerativeModel(p = [1.], sigmas = [.2,.4], coeffs = [[-.2,1,+.2,-.3], [-1,-.5,.9,.2]])\n",
    "# print(GroundTruth.coeffs) # Parameters of the model are easily accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample our \"ground truth\" distribution, and add some Gaussian noise\n",
    "(W_d, X_d) = GroundTruth(NPOINTS)\n",
    "X_d = X_d + .05 * Variable(torch.normal( means = torch.zeros(X_d.size()) ).type(dtype)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To display our point cloud, we first have to turn our\n",
    "# Pytorch variables (possibly GPU-enabled) into numpy arrays\n",
    "w_d = W_d.data.cpu().numpy() ; x_d = X_d.data.cpu().numpy()\n",
    "\n",
    "# We can then use standard matplotlib routines:\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax  = plt.subplot(1,1,1)\n",
    "ax.scatter( x_d[:,0], x_d[:,1], 6400*w_d, 'g', edgecolors='none' )\n",
    "plt.axis('equal')   ; plt.tight_layout()\n",
    "ax.set_xlim([-2,2]) ; ax.set_ylim([-2,2])\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The default model (starting point) v. our target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A \"default\" polynomial model of degree 3.\n",
    "MyModel = GenerativeModel(degree = 3)\n",
    "(W_t, X_t) = MyModel(NPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's display it next to our \"Ground Truth\"!\n",
    "w_d = W_d.data.cpu().numpy() ; x_d = X_d.data.cpu().numpy()\n",
    "w_t = W_t.data.cpu().numpy() ; x_t = X_t.data.cpu().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax  = plt.subplot(1,1,1)\n",
    "GroundTruth.plot(ax, 'g')\n",
    "MyModel.plot(ax, 'm')\n",
    "ax.scatter( x_d[:,0], x_d[:,1], 6400*w_d, 'b', edgecolors='none' )\n",
    "ax.scatter( x_t[:,0], x_t[:,1], 6400*w_t, 'r', edgecolors='none' )\n",
    "\n",
    "plt.axis('equal')   ; plt.tight_layout()\n",
    "ax.set_xlim([-2,2]) ; ax.set_ylim([-2,2])\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get prepared for model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_situation(fig, ax, ground_truth,     mymodel, \n",
    "                            training_measure, generated_measure,\n",
    "                            info_type,        info             ) :\n",
    "    \"\"\"\n",
    "    During the model-fitting loop, plots the evolution of the\n",
    "    trained model next to our \"ground truth\".\n",
    "    \"\"\"\n",
    "    # Turn PyTorch variables into numpy arrays --------------------------\n",
    "    w_d =  training_measure[0].data.cpu().numpy()\n",
    "    x_d =  training_measure[1].data.cpu().numpy()\n",
    "    w_t = generated_measure[0].data.cpu().numpy()\n",
    "    x_t = generated_measure[1].data.cpu().numpy()\n",
    "    \n",
    "    # Remove the colorbar if any, clear the axis ------------------------\n",
    "    if len(ax.images) > 0 :\n",
    "        cbars = ax.images[0].colorbar\n",
    "        if cbars is not None : \n",
    "            cbars.remove()\n",
    "    ax.clear()\n",
    "    \n",
    "    # Plot the \"info\" image/transport-plan, if any ----------------------\n",
    "    if info is not None :\n",
    "        info  = info.data.cpu().numpy()\n",
    "        if info_type == \"heatmap\" :     # The info is a signal defined on the plane\n",
    "            # We display it as a background image\n",
    "            scale = np.amax(np.abs(info)) * 1.5\n",
    "            cax   = ax.imshow(info, interpolation='bilinear', origin='lower', \n",
    "                      vmin = -scale, vmax = scale, cmap=cm.bwr, \n",
    "                      extent=(-2,2, -2, 2), zorder=-2)\n",
    "            cbar  = fig.colorbar(cax)\n",
    "        elif info_type == \"transport\" : # The info is a transport plan between the two measures\n",
    "            # We display it as a \"spider's web\" linking the training measure to the generated one.\n",
    "            segs = []\n",
    "            Q_weights,Q_points = w_t,x_t  ;  targetpoints = x_d\n",
    "            # (the code below is here to produce fancy plots, no need to worry about it)\n",
    "            for (a, mui, gi) in zip(Q_points, Q_weights, info) :\n",
    "                #segs += [ [a, targetpoints[np.argmax(gi)] ] ]\n",
    "                gi = gi / mui # gi[j] = fraction of the mass from \"a\" which goes to targetpoints[j]\n",
    "                for (xj, gij) in zip(targetpoints, gi) :\n",
    "                    mass_per_line = 0.02\n",
    "                    if gij >= mass_per_line :\n",
    "                        nlines = np.floor(gij / mass_per_line)\n",
    "                        ts     = np.linspace(-.005*(nlines-1), .005*(nlines-1), nlines)\n",
    "                        for t in ts :\n",
    "                            b = xj + t*np.array([ xj[1]-a[1], -xj[0]+a[0]])\n",
    "                            segs += [[a, b]]\n",
    "                            \n",
    "            line_segments = LineCollection(np.array(segs), linewidths=(.3,), \n",
    "                                           colors=[(.6,.8,1.)]*len(segs), linestyle='solid', zorder=-1)\n",
    "            ax.add_collection(line_segments)\n",
    "    \n",
    "    # Plot the model \"embeddings\", and the associated point clouds ------\n",
    "    ground_truth.plot(ax, 'g')\n",
    "    mymodel.plot(ax, 'm')\n",
    "    ax.scatter( x_d[:,0], x_d[:,1], 6400*w_d, 'b', edgecolors='none' )\n",
    "    ax.scatter( x_t[:,0], x_t[:,1], 6400*w_t, 'r', edgecolors='none' )\n",
    "    \n",
    "    # Ready to plot ! ---------------------------------------------------\n",
    "    plt.axis('equal')  ; plt.tight_layout() ; ax.set_xlim([-2,2]) ; ax.set_ylim([-2,2])\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FitModel(Model, Fidelity, EmpiricMeasure, name = \"model\", info_type = \"heatmap\", **kwargs) :\n",
    "    \"\"\"\n",
    "    Given an EmpiricMeasure (observed), and a Fidelity \"error assessment formula\"\n",
    "    betweens measures, this routines fits an arbitrary generative Model\n",
    "    to make it generate sample distributions close to the empirical one.\n",
    "    \"\"\"\n",
    "    # We'll minimize \"Fidelity( SampledModelDistribution, EmpiricMeasure )\"\n",
    "    # with respect to the model's parameters using a standard gradient-like\n",
    "    # descent scheme.\n",
    "    optimizer = torch.optim.Adam(Model.parameters(), lr = .05)\n",
    "    \n",
    "    # We'll plot results on-the-fly\n",
    "    fig = plt.figure(figsize=(10,8)) ; ax  = plt.subplot(1,1,1)\n",
    "    costs = [] ; N = len(EmpiricMeasure[0]) ; NLOG = 50\n",
    "    \n",
    "    FitModel.nit = -1\n",
    "    def closure():\n",
    "        \"Encapsulates the problem + display.\"\n",
    "        FitModel.nit += 1 ; i = FitModel.nit\n",
    "        # Minimization loop --------------------------------------------------------------------\n",
    "        optimizer.zero_grad()                      # Reset the gradients (PyTorch syntax...).\n",
    "        GeneratedMeasure = Model(N)                # Draw a random sample from our model.\n",
    "        Cost, info = Fidelity( GeneratedMeasure, EmpiricMeasure,  # Compute the discrepancy\n",
    "                               info = (i%NLOG==0), **kwargs )     # wrt. the empirical distrib.\n",
    "        costs.append(Cost.data.cpu().numpy()[0])   # Store the \"cost\" for plotting.\n",
    "        Cost.backward(retain_graph=True)           # Backpropagate to compute the gradient.\n",
    "        \n",
    "        if i % NLOG == 0: # Display the current model ------------------------------------------\n",
    "            print(\"Iteration \",i,\"Cost = \", Cost.data.cpu().numpy()[0])\n",
    "            plot_situation(fig, ax, GroundTruth,    Model, \n",
    "                                    EmpiricMeasure, GeneratedMeasure,\n",
    "                                    info_type, info                   )\n",
    "            fig.savefig('output/'+name+'_'+str(i)+'.png', dpi=fig.dpi) # -----------------------\n",
    "        \n",
    "        return Cost\n",
    "    \n",
    "    for i in range(401) :           # Fixed number of iterations\n",
    "        optimizer.step(closure)     # \"Gradient descent\" step.\n",
    "            \n",
    "        \n",
    "    # Once the minimization is over, display the cost evolution --------------------------------\n",
    "    fig = plt.figure(figsize=(8,8)) ;  ax  = plt.subplot(1,1,1)\n",
    "    ax.plot(np.array(costs))\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a kernel norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_scalar_product(Mu, Nu, mode = \"gaussian\", s = 1.) :\n",
    "    \"\"\"\n",
    "    Takes as input two measures Mu and Nu,\n",
    "    Mu = \\sum_i mu_i*\\delta_{x_i}  ; Nu = \\sum_j nu_j*\\delta_{y_j}\n",
    "    \n",
    "    as well as parameters for a kernel function.\n",
    "    \n",
    "    Computes the kernel scalar produc\n",
    "    <Mu,Nu>_k = < Mu, k \\star Nu >_{L^2}                   (convolution product)\n",
    "              = \\sum_{i,j} k(x_i-y_j) * mu_i * nu_j\n",
    "    \"\"\"\n",
    "    (mu, x) = Mu ; (nu, y) = Nu\n",
    "    k_nu = kernel_product(x,y,nu,mode,s)\n",
    "    return torch.dot( mu.view(-1), k_nu.view(-1)) # PyTorch syntax for a scalar product..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kernel_distance(Mu, Nu, info = False, mode = \"gaussian\", s = 1.) :\n",
    "    \"\"\"\n",
    "    Hilbertian kernel distance between measures Mu and Nu,\n",
    "    computed using the fact that\n",
    "    \n",
    "    |Mu-Nu|^2_k  =  <Mu,Mu>_k - 2 <Mu,Nu>_k + <Nu,Nu>_k\n",
    "    \n",
    "    If \"info\" is required, we output the values of\n",
    "         k \\star (Mu-Nu)  sampled on a uniform grid,\n",
    "    to be plotted later.\n",
    "    \n",
    "    Strictly speaking, it would make more sense to display\n",
    "         g \\star (Mu-Nu)     where     g \\star g = k\n",
    "    as we would then have\n",
    "          |Mu-Nu|^2_k  =  |g \\star (Mu-Nu)|^2_{L^2}.\n",
    "        \n",
    "    But this is easy only for Gaussians...\n",
    "    \"\"\"\n",
    "    D2 =   (   kernel_scalar_product(Mu,Mu,mode,s) \\\n",
    "           +   kernel_scalar_product(Nu,Nu,mode,s) \\\n",
    "           - 2*kernel_scalar_product(Mu,Nu,mode,s) )\n",
    "    \n",
    "    kernel_heatmap = None\n",
    "    if info :\n",
    "        # Create a uniform grid on the [-2,+2]x[-2,+2] square:\n",
    "        res    = 100 ; ticks = np.linspace( -2, 2, res + 1)[:-1] + 1/(2*res) \n",
    "        X,Y    = np.meshgrid( ticks, ticks )\n",
    "        points = Variable(torch.from_numpy(np.vstack( (X.ravel(), Y.ravel()) ).T).type(dtype), requires_grad=False)\n",
    "        \n",
    "        # Sample \"k \\star (Mu-Nu)\" on this grid:\n",
    "        kernel_heatmap   = kernel_product(points, Mu[1], Mu[0], mode, s) \\\n",
    "                         - kernel_product(points, Nu[1], Nu[0], mode, s)\n",
    "        kernel_heatmap   = kernel_heatmap.view(res,res) # reshape as a \"background\" image\n",
    "    return D2, kernel_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MyModel = GenerativeModel(degree = 3)\n",
    "FitModel(MyModel, kernel_distance, (W_d, X_d), mode = \"energy\", s=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an Optimal Transport distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wasserstein_distance(Mu, Nu, info = False, mode = \"gaussian\", s = .2) :\n",
    "    \"\"\"\n",
    "    Executes the Sinkhorn algorithm to compute an approximate Optimal Transport\n",
    "    cost between Mu and Nu, using an entropic regularization of strength\n",
    "    epsilon = s^2.\n",
    "    \"\"\"\n",
    "    A = Variable(torch.ones(Mu[0].size()).type(dtype))\n",
    "    B = Variable(torch.ones(Nu[0].size()).type(dtype))\n",
    "    for it in range(1000) :\n",
    "        A_prev = A\n",
    "        A = Mu[0] / kernel_product(Mu[1], Nu[1], B,mode,s).view(-1)\n",
    "        B = Nu[0] / kernel_product(Nu[1], Mu[1], A,mode,s).view(-1)\n",
    "        err = (A.log()-A_prev.log()).abs().mean().data.cpu().numpy()\n",
    "        #if err < 1e-5 :\n",
    "        #    print(it, \": \", err, \", \", end=\"\")\n",
    "        #    break\n",
    "        \n",
    "    #D2 = kernel_scalar_product((A,Mu[1]), (B,Nu[1]), mode, s)\n",
    "    eps = s**2\n",
    "    D2_d = eps * ( torch.dot(Mu[0].view(-1), A.log().view(-1) + .5) \\\n",
    "                 + torch.dot(Nu[0].view(-1), B.log().view(-1) + .5) )\n",
    "    \n",
    "    x_i = Mu[1].unsqueeze(1) ; y_j = Nu[1].unsqueeze(0)\n",
    "    xmy = ((x_i-y_j)**2).sum(2)\n",
    "    if   mode == \"gaussian\" : K = torch.exp( - xmy / (s**2) )\n",
    "    elif mode == \"laplace\"  : K = torch.exp( - torch.sqrt(xmy + (s**2)) )\n",
    "    elif mode == \"energy\"   : K = torch.pow( xmy + (s**2), -.25 )\n",
    "    transport_plan = K * (A[0].view(-1,1) * B[0].view(1,-1))\n",
    "    D2_p = eps * (transport_plan * xmy).sum()    \n",
    "    \n",
    "    print( (D2_p + D2_d).data.cpu().numpy() , \", \", end =\"\")\n",
    "        \n",
    "    transport_plan = None\n",
    "    if info :\n",
    "        x_i = Mu[1].unsqueeze(1) ; y_j = Nu[1].unsqueeze(0)\n",
    "        xmy = ((x_i-y_j)**2).sum(2)\n",
    "        if   mode == \"gaussian\" : K = torch.exp( - xmy / (s**2) )\n",
    "        elif mode == \"laplace\"  : K = torch.exp( - torch.sqrt(xmy + (s**2)) )\n",
    "        elif mode == \"energy\"   : K = torch.pow( xmy + (s**2), -.25 )\n",
    "        transport_plan = K #* (Mu[0].view(-1,1) * Nu[0].view(1,-1))\n",
    "        print((transport_plan.sum(1)-Mu[0]).abs().mean() / (Mu[0].abs().mean()))\n",
    "        print((transport_plan.sum(0)-Nu[0]).abs().mean() / (Nu[0].abs().mean()))\n",
    "    return D2_p, transport_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#MyModel = GenerativeModel(degree = 2)\n",
    "#FitModel(MyModel, wasserstein_distance, (W_d, X_d), \n",
    "#         info_type = \"transport\", name = \"OT\", mode=\"gaussian\", s = .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn  algorithm in the log domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(mat, dim):\n",
    "    max_rc = torch.max(mat, dim)[0]\n",
    "    return max_rc + torch.log(torch.sum(torch.exp(mat - max_rc.unsqueeze(dim)), dim))\n",
    "\n",
    "def wasserstein_distance_log(Mu, Nu, info = False, s = .2) :\n",
    "    \"\"\"\n",
    "    Log-domain implementation of the Sinkhorn algorithm,\n",
    "    for numerical stability.\n",
    "    The \"multiplicative\" standard implementation is replaced\n",
    "    by an \"additive\" logarithmic one, as:\n",
    "    - A is replaced by U = eps * log(A)\n",
    "    - B is replaced by V = eps * log(A)\n",
    "    - K_ij is replaced by C_ij = - eps * log(K_ij)\n",
    "                               = |X_i-Y_j|^2\n",
    "    (remember that epsilon = eps = s^2)\n",
    "    \n",
    "    The update step:\n",
    "    \n",
    "    \" a_i = mu_i / \\sum_j k(x_i,y_j) b_j \"\n",
    "    \n",
    "    is thus replaced, applying eps*log(...) on both sides, by\n",
    "    \n",
    "    \" u_i = eps*[ log(mu_i) - log(sum( exp(-C_ij/eps) * exp(v_j/eps) )) ]\n",
    "          = eps*[ log(mu_i) - log(sum( exp((v_j-C_ij)/eps ))            ]\n",
    "          = eps*[ log(mu_i) - log(sum( exp((u_i+v_j-C_ij)/eps))) + u_i/eps  ]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    U   = Variable(torch.zeros(Mu[0].size()).type(dtype))\n",
    "    V   = Variable(torch.zeros(Nu[0].size()).type(dtype))\n",
    "    C   = ((Mu[1].unsqueeze(1) - Nu[1].unsqueeze(0) )**2).sum(2).sqrt()\n",
    "    tau = -.5  # We use a slight extrapolation to go faster (this may let our algorithm diverge...)\n",
    "    eps = s#**2\n",
    "    for it in range(2000) :\n",
    "        U_prev = U\n",
    "        U = tau*U + (1-tau)*eps*( torch.log(Mu[0]) - log_sum_exp( (V.view(1,-1) - C) / eps, dim=1 ).view(-1) )\n",
    "        V = tau*V + (1-tau)*eps*( torch.log(Nu[0]) - log_sum_exp( (U.view(-1,1) - C) / eps, dim=0 ).view(-1) )\n",
    "                 \n",
    "        err = (U-U_prev).abs().mean().data.cpu().numpy()\n",
    "        if err < 1e-6 :\n",
    "            print(it, \", \", end=\"\")\n",
    "            break\n",
    "         \n",
    "    #D2 = (( (U.view(-1,1)+V.view(1,-1) - C) / eps ).exp() * C).sum()\n",
    "    D2 = ( torch.dot(Mu[0].view(-1), U.view(-1) + .5) \\\n",
    "         + torch.dot(Nu[0].view(-1), V.view(-1) + .5) )\n",
    "    transport_plan = None\n",
    "    if info :\n",
    "        transport_plan = ((U.view(-1,1)+V.view(1,-1) - C) / eps ).exp()\n",
    "    return D2, transport_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MyModel = GenerativeModel(degree = 2)\n",
    "FitModel(MyModel, wasserstein_distance_log, (W_d, X_d), info_type = \"transport\", name = \"wasserstein\", s = .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Optirun",
   "language": "python",
   "name": "optirun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
